\section{代码示例}

如代码1， 2，3 所示,分为三个部分mmdet.py, yolo.py, fastrcnn.py

\begin{lstlisting}[caption={mmdet.py}, label={lst:code-example}, captionpos=t, language=python]
    from mmdet.registry import VISUALIZERS
    import mmcv
    from mmdet.apis import init_detector, inference_detector
    import cv2

    # init the visualizer(execute this block only once)
    config_file = '../data/mmdet/rtmdet_tiny_8xb32-300e_coco.py'
    checkpoint_file = '../data/mmdet/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'
    model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'
    visualizer = VISUALIZERS.build(model.cfg.visualizer)
    # the dataset_meta is loaded from the checkpoint and
    # then pass to the model in init_detector
    visualizer.dataset_meta = model.dataset_meta

    # show the results
    for i in range(4):
        img = mmcv.imread(f'../data/{i}.jpg', channel_order='rgb')
        result = inference_detector(model, img)
        visualizer.add_datasample(
            'result',
            img,
            data_sample=result,
            draw_gt=False,
            wait_time=0,
        )
        visualizer.show()
        visualized_img = visualizer.get_image()
        cv2.imwrite(f'../data/mmdet/{i}_result.jpg', visualized_img)
\end{lstlisting}

\begin{lstlisting}[caption={yolo.py}, label={lst:code-example}, captionpos=t, language=python]
    from ultralytics import YOLO

    # Create a new YOLO model from scratch
    model = YOLO("yolo11.yaml")

    # Load a pretrained YOLO model (recommended for training)
    model = YOLO("yolo11n.pt")

    # Train the model using the 'coco8.yaml' dataset for 3 epochs
    results = model.train(data="coco8.yaml", epochs=3)

    # Evaluate the model's performance on the validation set
    results = model.val()

    # Perform object detection on an image using the model
    for i in range(4):
        results = model(f"../data/{i}.jpg")
        results[0].save(f'../data/yolo/{i}_result.jpg')

    # Export the model to ONNX format
    success = model.export(format="onnx")
\end{lstlisting}

\begin{lstlisting}[caption={fastrcnn.py}, label={lst:code-example}, captionpos=t, language=python]
    import torch
    import torchvision
    from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights
    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
    from torchvision.transforms import functional as F
    from PIL import Image

    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT 
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)

    print(model)
    num_classes = 2 
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    model.eval()
    dummy_image = torch.rand(1, 3, 800, 800)

    img_path = "../data/3.jpg"
    img = Image.open(img_path).convert("RGB")
    preprocess = weights.transforms()
    image_tensor = preprocess(img).unsqueeze(0) # Add batch dimension

    with torch.no_grad(): 
        predictions = model(image_tensor) 
        print(predictions[0])
\end{lstlisting}